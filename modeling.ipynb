{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pydataset import data\n",
    "\n",
    "import pyspark\n",
    "import pyspark.ml\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('case.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Rename column\n",
    "df = df.withColumnRenamed('SLA_due_date', 'case_due_date')\n",
    "\n",
    "# Convert to better data types\n",
    "df = (\n",
    "    df.withColumn('case_late', col('case_late') == 'YES')\n",
    "    .withColumn('case_closed', col('case_closed') == 'YES')\n",
    ")\n",
    "df = df.withColumn('council_district', format_string('%04d', col('council_district')))\n",
    "df = (\n",
    "    df.withColumn('case_opened_date', to_timestamp(col('case_opened_date'), 'M/d/yy H:mm'))\n",
    "    .withColumn('case_closed_date', to_timestamp(col('case_closed_date'), 'M/d/yy H:mm'))\n",
    "    .withColumn('case_due_date', to_timestamp(col('case_due_date'), 'M/d/yy H:mm'))\n",
    ")\n",
    "\n",
    "# Cleanup text data\n",
    "df = df.withColumn('request_address', lower(trim(col('request_address'))))\n",
    "# Extract zipcode\n",
    "df = df.withColumn('zipcode', regexp_extract(col('request_address'), r'\\d+$', 0))\n",
    "\n",
    "# Create a `case_lifetime` feature\n",
    "df = (\n",
    "    df.withColumn('case_age', datediff(current_timestamp(), 'case_opened_date'))\n",
    "    .withColumn('days_to_closed', datediff('case_closed_date', 'case_opened_date'))\n",
    "    .withColumn('case_lifetime', when(col('case_closed'), col('days_to_closed')).otherwise(col('case_age')))\n",
    "    .drop('case_age', 'days_to_closed')\n",
    ")\n",
    "\n",
    "# Join departments and sources\n",
    "depts = spark.read.csv('dept.csv', header=True, inferSchema=True)\n",
    "sources = spark.read.csv('source.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df.join(depts, 'dept_division', 'left').join(sources, 'source_id', 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "Use the .randomSplit method to split the 311 data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "Create a classification model to predict whether a case will be late or not (i.e. predict case_late). Experiment with different combinations of features and different classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " source_id              | 100137               \n",
      " dept_division          | 311 Call Center      \n",
      " case_id                | 1014263399           \n",
      " case_opened_date       | 2018-02-21 15:36:00  \n",
      " case_closed_date       | 2018-02-21 15:38:00  \n",
      " case_due_date          | 2018-03-02 15:36:00  \n",
      " case_late              | false                \n",
      " num_days_late          | -8.998854167000001   \n",
      " case_closed            | true                 \n",
      " service_request_type   | Compliment           \n",
      " SLA_days               | 9.0                  \n",
      " case_status            | Closed               \n",
      " request_address        | 927  donaldson av... \n",
      " council_district       | 0007                 \n",
      " zipcode                | 78228                \n",
      " case_lifetime          | 0                    \n",
      " dept_name              | Customer Service     \n",
      " standardized_dept_name | Customer Service     \n",
      " dept_subject_to_SLA    | YES                  \n",
      " source_username        | Merlene Blodgett     \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(1,vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " source_id              | 100137               \n",
      " dept_division          | 311 Call Center      \n",
      " case_id                | 1014263399           \n",
      " case_opened_date       | 2018-02-21 15:36:00  \n",
      " case_closed_date       | 2018-02-21 15:38:00  \n",
      " case_due_date          | 2018-03-02 15:36:00  \n",
      " case_late              | false                \n",
      " num_days_late          | -8.998854167000001   \n",
      " case_closed            | true                 \n",
      " service_request_type   | Compliment           \n",
      " SLA_days               | 9.0                  \n",
      " case_status            | Closed               \n",
      " request_address        | 927  donaldson av... \n",
      " council_district       | 0007                 \n",
      " zipcode                | 78228                \n",
      " case_lifetime          | 0                    \n",
      " dept_name              | Customer Service     \n",
      " standardized_dept_name | Customer Service     \n",
      " dept_subject_to_SLA    | YES                  \n",
      " source_username        | Merlene Blodgett     \n",
      " features               | (333,[147],[1.0])    \n",
      " label                  | 0.0                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = pyspark.ml.feature.RFormula(formula='case_late ~ service_request_type').fit(train)\n",
    "train_input = rf.transform(train)\n",
    "train_input.show(1,vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8162039773262637"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = pyspark.ml.classification.LogisticRegression()\n",
    "lr_fit = lr.fit(train_input)\n",
    "lr_fit.summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "Create a regression model to predict how many days late a case will be (i.e. predict num_days_late). Experiment with different combinations of features and different regression algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = pyspark.ml.feature.RFormula(formula=\"num_days_late ~ case_lifetime + SLA_days\").fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------\n",
      " features | [0.0,9.0]          \n",
      " label    | -8.998854167000001 \n",
      "-RECORD 1----------------------\n",
      " features | [1.0,14.0053125]   \n",
      " label    | -12.64164352       \n",
      "-RECORD 2----------------------\n",
      " features | [2.0,14.0]         \n",
      " label    | -12.32291667       \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_input = rf.transform(train).select('features', 'label')\n",
    "train_input.show(3,vertical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_22ca386e6e2a"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = pyspark.ml.regression.LinearRegression()\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
